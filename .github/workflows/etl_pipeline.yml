name: ETL Pipeline - Generate, Upload & Deploy

# Triggers
on:
  push:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    - cron: '0 1 * * *'   # Daily at 1 AM UTC

jobs:
  generate-and-upload:
    name: Generate CSVs, Upload to S3, Deploy Lambda & RDS
    runs-on: ubuntu-latest

    steps:
    # Step 1: Checkout repository
    - name: 1.1 Checkout repository
      uses: actions/checkout@v4

    # Step 2: Setup Python
    - name: 1.2 Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    # Step 3: Install dependencies
    - name: 1.3 Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libpq-dev gcc postgresql-client
        python -m pip install --upgrade pip
        pip install faker pandas boto3 awscli psycopg2-binary

    # Step 4: Debug environment
    - name: 1.4 Debug environment
      run: |
        python --version
        aws --version
        psql --version
        ls -R

    # Step 5: Run data generator
    - name: 1.5 Generate 90-days sales CSVs
      run: |
        python data_generator/generate_3months_data.py

    # Step 6: Verify generated files
    - name: 1.6 List generated CSV files
      run: |
        echo "Listing CSV files:"
        find sample_data -type f -name "*.csv" -print || true
        echo "Total files:"
        find sample_data -type f -name "*.csv" | wc -l || true

    # Step 7: Configure AWS CLI
    - name: 2.1 Configure AWS CLI
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    # Step 8: Upload CSVs to S3
    - name: 2.2 Upload CSVs to S3
      run: |
        for file in sample_data/*.csv; do
          echo "Uploading $file to S3 bucket ${{ secrets.S3_BUCKET_NAME }}"
          aws s3 cp "$file" s3://${{ secrets.S3_BUCKET_NAME }}/
        done

    # Step 9: Verify S3 upload
    - name: 2.3 Verify S3 upload
      run: aws s3 ls s3://${{ secrets.S3_BUCKET_NAME }}/

    # Step 10: Deploy Lambda (optional)
    - name: 3.1 Package and deploy Lambda
      if: ${{ secrets.LAMBDA_FUNCTION_NAME != '' }}
      run: |
        zip -r lambda_function.zip lambda/lambda_function.py
        aws lambda update-function-code \
          --function-name ${{ secrets.LAMBDA_FUNCTION_NAME }} \
          --zip-file fileb://lambda_function.zip

    # Step 11: Load CSVs to RDS (optional)
    - name: 4.1 Load CSV to RDS
      if: ${{ secrets.RDS_HOST != '' }}
      run: |
        for file in sample_data/*.csv; do
          echo "Loading $file into RDS table sales_table"
          PGPASSWORD=${{ secrets.RDS_PASSWORD }} psql \
            -h ${{ secrets.RDS_HOST }} \
            -U ${{ secrets.RDS_USER }} \
            -d ${{ secrets.RDS_DB }} \
            -c "\copy sales_table FROM '$file' DELIMITER ',' CSV HEADER;"
        done

    # Step 12: Verify RDS load (optional)
    - name: 4.2 Verify RDS data
      if: ${{ secrets.RDS_HOST != '' }}
      run: |
        PGPASSWORD=${{ secrets.RDS_PASSWORD }} psql \
          -h ${{ secrets.RDS_HOST }} \
          -U ${{ secrets.RDS_USER }} \
          -d ${{ secrets.RDS_DB }} \
          -c "SELECT COUNT(*) FROM sales_table;"
