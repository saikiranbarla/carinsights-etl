name: ETL Pipeline - Generate, Upload & Deploy

# Triggers
on:
  push:
    branches: [ main ]      # Trigger on push to main branch
  workflow_dispatch:         # Manual trigger
  schedule:                  # CRON schedule: daily at 1 AM UTC
    - cron: '0 1 * * *'

jobs:
  generate-and-upload:
    name: Generate CSVs, Upload to S3, Deploy Lambda & RDS
    runs-on: ubuntu-latest

    steps:
    # Step 1: Checkout repository
    - name: 1.1 Checkout repository
      uses: actions/checkout@v4

    # Step 2: Setup Python
    - name: 1.2 Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    # Step 3: Install Python dependencies
    - name: 1.3 Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install faker pandas boto3 awscli psycopg2-binary

    # Step 4: Run data generator script
    - name: 1.4 Generate 90-days sales CSVs
      run: |
        python data_generator/generate_3months_data.py

    # Step 5: List generated files for verification
    - name: 1.5 List generated CSV files
      run: |
        echo "Listing generated CSV files:"
        find sample_data -type f -name "*.csv" -print || true
        echo "Total files:"
        find sample_data -type f -name "*.csv" | wc -l || true

    # Step 6: Configure AWS CLI
    - name: 2.1 Configure AWS CLI
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    # Step 7: Upload CSV files to S3
    - name: 2.2 Upload CSVs to S3
      run: |
        for file in sample_data/*.csv; do
          echo "Uploading $file to S3 bucket ${{ secrets.S3_BUCKET_NAME }}"
          aws s3 cp "$file" s3://${{ secrets.S3_BUCKET_NAME }}/
        done

    # Step 8: Verify S3 upload
    - name: 2.3 Verify S3 upload
      run: |
        aws s3 ls s3://${{ secrets.S3_BUCKET_NAME }}/

    # Step 9: Deploy Lambda function (optional)
    - name: 3.1 Package and deploy Lambda
      if: ${{ secrets.LAMBDA_FUNCTION_NAME != '' }}
      run: |
        zip -r lambda_function.zip lambda/lambda_function.py
        aws lambda update-function-code \
          --function-name ${{ secrets.LAMBDA_FUNCTION_NAME }} \
          --zip-file fileb://lambda_function.zip

    # Step 10: Optional: Load CSV to RDS PostgreSQL
    - name: 4.1 Load CSV to RDS
      if: ${{ secrets.RDS_HOST != '' }}
      run: |
        for file in sample_data/*.csv; do
          echo "Loading $file into RDS table sales_table"
          PGPASSWORD=${{ secrets.RDS_PASSWORD }} psql \
            -h ${{ secrets.RDS_HOST }} \
            -U ${{ secrets.RDS_USER }} \
            -d ${{ secrets.RDS_DB }} \
            -c "\copy sales_table FROM '$file' DELIMITER ',' CSV HEADER;"
        done

    # Step 11: Verify RDS data (optional)
    - name: 4.2 Verify RDS load
      if: ${{ secrets.RDS_HOST != '' }}
      run: |
        PGPASSWORD=${{ secrets.RDS_PASSWORD }} psql \
          -h ${{ secrets.RDS_HOST }} \
          -U ${{ secrets.RDS_USER }} \
          -d ${{ secrets.RDS_DB }} \
          -c "SELECT COUNT(*) FROM sales_table;"
